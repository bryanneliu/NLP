# Build lemmatization vocab to fa_lex:
cat amazon_lemma_en.csv | awk 'BEGIN {FS="\t"} NF == 3 {print "< ^ " $1 " \$ > --> " $3}' > lemma.vocab.falex
cat amazon_lemma_en.csv | awk 'BEGIN {FS="\t"} NF == 3 && !seen[$3]++ {print $3 " " NR+500}' > lemma.tagset.txt

# There is no namespace in falex
# define variable once and used it after the definition


# When cat file generated in Windows and file generated in Linux, it will generates ^M at the end of the row:
awk '{ gsub(/\r/, ""); print }' tagset.windows.txt > tagset.linux.txt
cat -v tagset.linux.txt lemma.tagset.txt > wbd.tagset.txt

# printf: escape % using %%, escape $ using \$
dev-dsk-lbryanne-2b-80ed3fb7 % printf "into is 24-packs of gardeningis ! gardening. 6%% \$25 7+ thereis is a" | fa_lex --ldb=ldb/tokenizer.bin --tagset=sp_sourcing/tokenizer/wbd.tagset.txt 
into/IGNORE is/IGNORE 24/MEASUREMENT -/IGNORE packs/UNIT_SET of/IGNORE gardeningis/WORD !/WORD gardening/garden ./WORD 6/WORD %/WORD $/WORD 25/WORD 7/WORD +/WORD thereis/WORD is/IGNORE a/IGNORE


# Build
(23-03-21 0:13:07) <0> [~/BlingFire/ldbsrc] 
make -f Makefile.gnu lang=sp_sourcing/tokenizer clean
make -f Makefile.gnu lang=sp_sourcing/tokenizer all

# Test
printf "24 packs" | fa_lex --ldb=ldb/tokenizer.bin --tagset=sp_sourcing/tokenizer/wbd.tagset.txt

# Test in scale
# Use : as a delimiter, paste -d :
paste <(cat sp_sourcing/tokenizer/unit.test) <(cat sp_sourcing/tokenizer/unit.test | fa_lex --ldb=ldb/tokenizer.bin --tagset=sp_sourcing/tokenizer/wbd.tagset.txt --normalize-input)

# Data Processing
# Check the line in a file
awk 'NR == 43420599' ../../work/data/cap_not_matched_close_match/query

# Remove the line with only "\n" or "\r\n":
awk 'length > 1' query > query.2

# Remove the leading and trailing quotes
awk -F'"' '{OFS="";gsub(/^"|"$/,""); print}' title.2 > title.3

# Remove accents
cat title | iconv -t ASCII//TRANSLIT > title2

# Change all chars to lower cases
cat title | awk '{print tolower($0)}' > title2

# For bebé, to remove diacritics, https://github.com/Microsoft/BlingFire/wiki/How-to-add-a-new-BERT-tokenizer-model
# Generate charmap.utf8 file using UTF-16 or UTF-32 encoding
UTF-8 Encoding:	0xC3 0xA9
UTF-16 Encoding:	0x00E9
UTF-32 Encoding:	0x000000E9

# to apply charmap, use --normalize-input
dev-dsk-lbryanne-2b-80ed3fb7 % printf "sofÁ" | fa_lex --ldb=ldb/analyzer.bin --tagset=sp_sourcing/analyzer/wbd.tagset.txt --normalize-input
sofa/WORD
